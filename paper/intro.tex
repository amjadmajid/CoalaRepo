Advances in processor efficiency along with the development of energy-harvesting power systems has created a new category of devices that require neither a battery nor a tethered power supply~\cite{prasad_comst_2014,lucia_snapl_2017,soyata_csm_2016}. These devices operate entirely using energy extracted from their environment, such as RF energy~\cite{rf_powered_computing_gollakota_2014}, photovoltaic components~\cite{margolies_infocom_2016,margolies_tosn_2016}, and vibration~\cite{gorlatova_sigmetrics_2014}. Incorporating compute, storage, sensing, and communication hardware~\cite{wisp5,moo}, such energy-independent devices are a promising underlying technology for applications in emerging areas such as the Internet of Things (IoT)~\cite{ku_cst_2016}, in-body~\cite{nadeau_naturebio_2017} and on-body~\cite{bandodkar_electroanalysis_2015} medical systems, and energy-harvesting nano-satellites~\cite{kicksat}.

While promising, energy-harvesting devices present a unique design challenge in that they operate only {\em intermittently} when energy is available~\cite{hicks_isca_2017,lucia_snapl_2017}. An
energy-harvesting device buffers energy from its environment, e.g., in a capacitor~\cite[Fig. 3]{gorlatova_tmc_2013},~\cite[Fig. 1]{gunduz_commag_2014}. When a threshold quantity of energy accumulates, the device begins operating. Harvestable energy sources typically produce little power compared to a platform's operating power level. A device operates for a brief period of time and when buffered energy is depleted, shuts down and begins recharging to operate again later. As an example, inter-task time may even exceed 20\,s for a wireless energy-harvesting battery-less vivo digestive tract monitoring system~\cite[Fig. 3c]{nadeau_naturebio_2017}. On top of that, charge and discharge times vary by device (different energy buffer sizes) and some of them~\cite{wisp} may fail $\approx$10 to $\approx$100 times per second~\cite[Fig. 1]{tan_infocom_2016},~\cite[Fig. 1]{mementos},~\cite[Fig. 3]{nvp}.

%---compare for instance~\cite[Table III and V]{prasad_comst_2014} with~\cite[Table I]{carrano_cst_2014}

Software in an energy-harvesting system operates according to the {\em intermittent execution model}~\cite{dino,lucia_snapl_2017}, with bursts of execution interrupted by failure periods. When power fails, a device loses its volatile state, i.e., registers, stack, SRAM, and retains its non-volatile state, i.e., FRAM, Flash. While capturing periodic checkpoints~\cite{mementos,quickrecall} and sleep scheduling~\cite{dewdrop,hibernus,hibernusplusplus} help to preserve execution progress, failures can leave volatile and non-volatile state inconsistent, leading to unrecoverable failures~\cite{mspcdino,edb}. 

There are two main approaches for dealing with data inconsistency for intermittently-powered devices: (i) programming and execution models~\cite{dino,ratchet,chain,alpaca} and (ii) architecture
mechanisms~\cite{hicks_isca_2017,idetic,nvp,tictpl}. While both approaches represent progress, each has critical limitations. New architectures require costly hardware changes and are inapplicable to today's embedded off-the-shelf systems~\cite[Fig. 3]{hicks_isca_2017}~\cite[Fig. 9]{nvp}. New programming models and compilers require programmers to change behavior~\cite{chain,ratchet}, inhibiting adoption.  New memory models may require more accesses to non-volatile memory (for multi-versioning)~\cite{dino,chain} or may preclude the use of volatile memory~\cite{ratchet}. Effective use of both volatile and non-volatile memory is important for \emph{efficiency}, because non-volatile memory has higher access energy and latency than SRAM~\cite[Fig. 4]{nvp}, and \emph{generality}, because devices often have much more FRAM than SRAM (e.g., 64 times more in~\cite{wolverine}). Hardware is effective, but inapplicable to existing devices, making programming and execution models essential to the success of intermittent computing.

Recent work~\cite{alpaca,chain} proposed {\em task-based} programming and execution models in which programmers statically decompose their program into a collection of tasks. Tasks can include arbitrary computation and, upon completion, are guaranteed to have executed {\em atomically}, despite arbitrarily-timed power failures. The programmer explicitly expresses task-to-task control flow, which may be conditionally input-dependent~\cite[Fig. 4]{chain}. An important programming challenge presented by a task-based model is that the length of a software task's execution is limited by the fixed, total amount of energy that a device can buffer in hardware.  A task's code is static, but the duration of its execution may be input-dependent and is difficult to predict. Assuming that input power is negligible compared to operating power~\cite{wisp}, a task will never be able to complete if its execution consumes more energy than the system can buffer, creating the potential for long tasks to be irreparably non-terminating. On the other hand, transitioning from one task to another imposes a run time cost and excessively small tasks are inefficient. 

The key research question addressed by this work is to determine how to define and execute tasks efficiently and ensures non-termination, given an unknown energy buffer size and energy arrival process. In particular, we ask (i) how to use software support to dynamically adapt the effective size of a task, while respecting programmer-specified task atomicity, and (ii) how to minimize run time and energy consumption while automatically maintaining memory consistency during execution?

\todo{This is the main pitch statement: we should all agree that this is what \sys does}{All} In this work we develop {\bf \sys}: a new programming and execution model that efficiently executes tasks and avoids non-termination across a range of energy buffer sizes, without burdening the programmer. To accomplish this goal, \sys introduces three new capabilities: 

\begin{itemize}
	\item {\bf Automatic task compilation:} \sys's compiler transforms arbitrary C code into a graph of tasks that reflects the original program's control- and data-flow constraints. The compiler identifies data shared by multiple of the newly created tasks and instruments reads and writes of those data so that \sys's memory virtualization mechanism keeps those data consistent. \sys complier effectively circumvents the need for labour-intensive manual task generation of the state-of-the-art solutions~\cite{chain,alpaca}. \todo{Elaborate on this more and provide numerical results}{Przemek}
	%
	\item {\bf Dynamic task coalescing:} \sys's memory virtualization mechanism uses SRAM as working memory, which \sys dynamically populates with pages of data from FRAM on demand during a task's execution. When a task ends, \sys commits dirty pages to FRAM using efficient DMA block copies, ensuring task atomicity and memory consistency. \todo{Once paging is fully implemented--update the paragraph}{Przemek}\todo{Provide numerical results for this section}{Przemek}
	%
	\item {\bf Volatile memory virtualization:} \sys's task coalescing mechanism executes multiple statically defined tasks together as a single dynamic task. A programmer can specify small tasks that will execute on a device with a small energy buffer. Coalescing tasks allows running the same tasks on a device with a larger energy buffer, while avoiding run time overheads associated with ending one task and beginning another.\todo{Provide numerical results for this section}{Przemek}
\end{itemize}

We evaluated \sys end-to-end on a collection of existing benchmark programs~\cite[Sec. 5]{chain} and new ones in a laboratory environment, and compare directly to state-of-the art task-based runtime~\cite{chain}, showing that \sys has high performance and can flexibly target a variety of platforms without recompiling the code. \todo{add detailed text about evaluation results}{Przemek}

%\todo{I think these are all much too long, given the depth of the intro and background already. I suggest we cut to the barest minimum.}

%\sys's core contribution is the introduction of dynamic task coalescing mechanism. It allows the programmer or a compiler to intuitively decompose the program into small tasks that amortize fixed per-task overheads, yet present no risk of exceeding device energy capacity. As such a decomposition executes, \sys~{\em coalesces dynamically} consecutive tasks. Coalescing elides the commits of coalesced tasks by buffering multiple tasks' updates in an FRAM commit buffer. Periodically, as the span of the coalesced task grows, \sys ends coalescing and commits the state of the coalesced task. If a power failure interrupts a sequence of coalesced tasks, \sys adaptively reduces the number of tasks in that sequence that it will coalesce, committing sooner in future executions. Consequently, \sys's coalescing mechanism allows a program to execute efficiently across a range of energy buffer sizes, avoiding transition overheads in larger buffers, and ensuring progress in smaller buffers.

%We propose and explore two data swapping strategies for intermittently-powered systems based on FRAM/SRAM architecture. The first strategy is {\em demand paging}, which swaps the SRAM page with a new page from FRAM, buffering the swapped-out page until commit. The second strategy is {\em buffered direct access}, which directly reads and writes FRAM relying on dynamic double-buffering to ensure memory is consistent at commit.
