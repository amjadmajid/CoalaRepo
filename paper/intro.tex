Advances in processor efficiency along with the development of energy-harvesting systems has created a new category of devices that require neither a battery nor a tethered power supply~\cite{prasad_comst_2014,lucia_snapl_2017,soyata_csm_2016}. These devices operate using energy extracted from their environment, such as RF~\cite{rf_powered_computing_gollakota_2014}, photovoltaics~\cite{margolies_infocom_2016,margolies_tosn_2016}, and vibration~\cite{gorlatova_sigmetrics_2014}. Incorporating compute, storage, sensing, and communication hardware~\cite{wisp5,moo}, such devices are a promising technology for use in the Internet of Things (IoT)~\cite{ku_cst_2016}, in-body~\cite{nadeau_naturebio_2017} and on-body~\cite{bandodkar_electroanalysis_2015} medical systems, and energy-harvesting nano-satellites~\cite{kicksat}.

Energy-harvesting devices create unique challenges because they operate {\em intermittently} when energy is available~\cite{hicks_isca_2017,lucia_snapl_2017}. An
energy-harvesting device buffers energy in a capacitor~\cite{gorlatova_tmc_2013},~\cite{gunduz_commag_2014} and when a threshold amount of energy accumulates, begins operating. Harvestable energy sources are low-power compared to a platform's operating level. A device operates briefly and when buffered energy is depleted, shuts down and recharges to operate again later. As an example, recharge time may be tens of seconds in RF powered medical device~\cite[Fig. 3c]{nadeau_naturebio_2017}. Charge and discharge times vary by device (different capacitor sizes) and some~\cite{wisp} may fail $\approx$10 to $\approx$100 times per second~\cite{tan_infocom_2016},~\cite{mementos},~\cite{nvp}.

Software in an energy-harvesting system operates in the {\em intermittent execution model}~\cite{dino,lucia_snapl_2017}, with execution interrupted by failure periods. When power fails, a device loses volatile state, i.e., registers, stack, SRAM, and retains its non-volatile state, i.e., FRAM, Flash. While capturing periodic checkpoints~\cite{mementos,quickrecall} and sleep scheduling~\cite{dewdrop,hibernus,hibernusplusplus} help preserve execution progress, failures can leave volatile and non-volatile state inconsistent, leading to unrecoverable failures~\cite{dino,edb}. 

There are two main approaches for dealing with data inconsistency for intermittently-powered devices: (i) programming and execution models~\cite{dino,ratchet,chain,alpaca} and (ii) architectures~\cite{hicks_isca_2017,idetic,nvp,tictpl}. While both approaches represent progress, each has limitations. New architectures require hardware changes and are inapplicable to today's systems~\cite{hicks_isca_2017,nvp}. New programming models and compilers require programmers to change behavior~\cite{chain,ratchet}, inhibiting adoption. New memory models may require more accesses to non-volatile memory (for multi-versioning)~\cite{dino,chain} or may preclude the use of volatile memory~\cite{ratchet}. Effective use of both volatile and non-volatile memory is important for \emph{efficiency}, because non-volatile memory has higher access energy and latency than SRAM~\cite{nvp}, and \emph{generality}, because devices often have much more FRAM than SRAM (e.g., 64 times more in~\cite{wolverine}). 

Recent work~\cite{alpaca,chain} proposed {\em task-based} programming and execution models in which programmers statically decompose a program into a collection of tasks. Tasks can include arbitrary computation and, upon completion, are guaranteed to have executed {\em atomically}, despite arbitrarily-timed power failures. The programmer explicitly expresses task-to-task control flow, which may be conditionally input-dependent~\cite{chain}. An key challenge is that the length of a software task's execution is limited by the fixed, total amount of energy that a device can buffer in hardware. A task's code is static, but the duration of its execution may be input-dependent and is difficult to predict. Assuming that input power is negligible compared to operating power~\cite{wisp}, a task will never be able to complete if its execution consumes more energy than the system can buffer. On the other hand, transitioning from one task to another imposes a run time cost and excessively small tasks are inefficient. 

The key research question addressed here is how to define and execute tasks efficiently and ensure non-termination, with an unknown energy buffer size and energy arrival process. In particular, we ask (i) how to use software support to \emph{dynamically adapt} the effective size of a task, while respecting programmer-specified task atomicity, and (ii) how to minimize run time and energy consumption while \emph{automatically maintaining memory consistency} during execution?

In this work we develop {\bf \sys}: a new programming and execution model efficiently executing tasks which avoids non-termination across a range of energy buffer sizes, without burdening the programmer. To accomplish this, \sys introduces new capabilities: 

\begin{itemize}
	\item {\bf Automatic task compilation:} \sys's compiler transforms arbitrary C code into a graph of tasks that reflects the original program's control- and data-flow constraints. The compiler identifies data shared by multiple of the newly created tasks and instruments reads and writes of those data so that \sys's memory virtualization mechanism keeps those data consistent. \sys's complier the labour-intensive process of writing tasks~\cite{chain,alpaca} (i.e., hours of programmer time).
	%
	\item {\bf Dynamic task coalescing:} \sys's task coalescing mechanism dynamically executes multiple tasks as a single task. A programmer or compiler can specify small tasks that will execute on a device with a small energy buffer. Coalescing tasks allows running the same tasks on a device with a larger energy buffer, avoiding run time overheads associated with ending one task and beginning another.
	%
	\item {\bf Volatile memory virtualization:} \sys's memory virtualization uses SRAM as working memory, which \sys dynamically populates with pages of data from FRAM on demand, during a task's execution. When a task ends, \sys commits dirty pages to FRAM using DMA block copies, ensuring task atomicity and memory consistency.
\end{itemize}

We evaluated \sys end-to-end on a collection of existing benchmarks ~\cite{chain} and new programs, and compare directly to a state-of-the-art task-based runtime~\cite{chain}, showing that \sys has high performance and can flexibly target a variety of platforms without rewriting code.

%\sys's core contribution is the introduction of dynamic task coalescing mechanism. It allows the programmer or a compiler to intuitively decompose the program into small tasks that amortize fixed per-task overheads, yet present no risk of exceeding device energy capacity. As such a decomposition executes, \sys~{\em coalesces dynamically} consecutive tasks. Coalescing elides the commits of coalesced tasks by buffering multiple tasks' updates in an FRAM commit buffer. Periodically, as the span of the coalesced task grows, \sys ends coalescing and commits the state of the coalesced task. If a power failure interrupts a sequence of coalesced tasks, \sys adaptively reduces the number of tasks in that sequence that it will coalesce, committing sooner in future executions. Consequently, \sys's coalescing mechanism allows a program to execute efficiently across a range of energy buffer sizes, avoiding transition overheads in larger buffers, and ensuring progress in smaller buffers.

%We propose and explore two data swapping strategies for intermittently-powered systems based on FRAM/SRAM architecture. The first strategy is {\em demand paging}, which swaps the SRAM page with a new page from FRAM, buffering the swapped-out page until commit. The second strategy is {\em buffered direct access}, which directly reads and writes FRAM relying on dynamic double-buffering to ensure memory is consistent at commit.
