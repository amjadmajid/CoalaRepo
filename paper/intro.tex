Advances in processor efficiency along with the development of energy-harvesting systems has created a new category of devices that require neither a battery nor a tethered power supply~\cite{prasad_comst_2014,lucia_snapl_2017,soyata_csm_2016}. These devices operate using ambient energy, such as radio frequency transmissions~\cite{rf_powered_computing_gollakota_2014}, light~\cite{margolies_infocom_2016,margolies_tosn_2016}, and vibration~\cite{gorlatova_sigmetrics_2014}. Incorporating compute, storage, sensing, and communication hardware~\cite{wisp5,moo}, such devices are a promising technology for use in the Internet of Things~\cite{ku_cst_2016}, in-body~\cite{nadeau_naturebio_2017} and on-body~\cite{bandodkar_electroanalysis_2015} medical systems, and energy-harvesting nano-satellites~\cite{kicksat}.

Energy-harvesting devices create unique challenges because they operate {\em intermittently} when energy is available~\cite{hicks_isca_2017,lucia_snapl_2017}. An
energy-harvesting device buffers energy in a small storage, e.g. a capacitor~\cite{gorlatova_tmc_2013},~\cite{gunduz_commag_2014} and when a threshold amount of energy accumulates, begins operating. Harvestable energy sources are low-power compared to a platform's operating level. A device operates briefly and when buffered energy is depleted, shuts down and recharges to operate again later. As an example, recharge time may be tens of seconds in radio frequency powered medical device~\cite[Fig. 3c]{nadeau_naturebio_2017}. Moreover, charge and discharge times vary by device (due to different capacitor sizes) and some may fail $\approx$10 to $\approx$100 times per second~\cite{tan_infocom_2016},~\cite{mementos},~\cite{nvp}.

\begin{figure}
	\begin{subfigure}[t]{.35\linewidth}
		\centering\large Figure
		\caption{\centering WISP~\cite{wisp} against an RFID antenna}\label{fig:1a}
	\end{subfigure}%
	\begin{subfigure}[t]{.65\linewidth}
		\centering\large Figure
		\caption{\centering Program X~\cite{hicks_mibench2_2016} (split into X and Y tasks~\cite{chain}) at two distances}\label{fig:1b}
    \end{subfigure}
	\caption{Impact of an inadequate task sizing on the computation speed of a moving transiently-powered device.}\label{fig:1}
\end{figure}

\textbf{Data Consistency in Intermittent Computing:} Software in an energy-harvesting system operates in the {\em intermittent execution model}~\cite{dino,lucia_snapl_2017}, with execution interrupted by failure periods. When power fails, a device loses volatile state, i.e., registers, stack, SRAM, and retains its non-volatile state, i.e., FRAM, Flash. While capturing periodic checkpoints~\cite{mementos,quickrecall} and sleep scheduling~\cite{dewdrop,hibernus,hibernusplusplus} help preserve execution progress, failures can leave volatile and non-volatile state inconsistent, leading to unrecoverable failures~\cite{dino,edb}. 

There are two main approaches for dealing with data inconsistency for intermittently-powered devices: (i) \emph{programming and execution models}~\cite{dino,ratchet,chain,alpaca} and (ii) \emph{architectures}~\cite{hicks_isca_2017,idetic,nvp,tictpl}. New architectures require hardware changes and are inapplicable to today's systems~\cite{hicks_isca_2017,nvp}, therefore new programming models and compilers are more suitable (as of now). However, they have their own limitations that need to be addressed---the core one being \emph{inability to adapt} to changing energy conditions.

\textbf{The Need for Adaptive Task Division:} Recently proposed {\em task-based} programming and execution models~\cite{alpaca,chain} advocate for static decomposition of a program (by a programmer) into a collection of tasks. Tasks can include arbitrary computation and, upon completion, are guaranteed to have executed {\em atomically}, despite arbitrarily-timed power failures. The programmer explicitly expresses task-to-task control flow, which may be conditionally input-dependent. A key challenge is that the length of a software task's execution is \emph{limited by the fixed total amount of energy} that a device can buffer in hardware. A task's code is static, but the duration of its execution may be input-dependent and is difficult to predict. To illustrate this, we refer to Figure~\ref{fig:1} showing the execution time of two applications (the same application X, but divided into X and Y tasks as in~\cite{chain}) running on RFID antenna-powered Computational RFID~\cite{wisp,rf_powered_computing_gollakota_2014} at two device-to-antenna distances (near---stable energy supply/far---high energy intermittency). At short distance: execution will takes too long caused by runtime cost of marshalling excessively small tasks. At far distance: program \emph{might never execute} if task execution consumes more energy than the system can buffer. This calls for at-runtime adaptive task division of any transiently-powered application---namely \emph{task virtualization}.

\begin{figure}
	\includegraphics[width=.5\columnwidth]{figures/alpaca-war.pdf}
	\caption{\label{fig:virtualization}Adaptive task division at run-time might lead to write-after-read dependencies (variable \ttfamily{x}) that do not exist with static task division.\todo{Box with Task 1,2,3 must be the same size as merged task; Merged task must be named as Task 4; x must be marked in text as having WAR; "Task X" name should be next to the task box; "Window" - small letter}{Sinan}}
\end{figure}

\textbf{The Challenge of Task Virtualization:} Task-based model which allows for dynamic task division must implement \emph{task virtualization mechanism}: merging of atomic, data-consistent tasks into a single (larger) task. This however might introduce \emph{write-after-read (WAR)} dependencies on the variables in non-volatile memory at \emph{run-time} that can break the atomicity of the \emph{virtual task}. Consider the example depicted in Fig.~\ref{fig:virtualization}: Tasks 1, 2 and 3 being executed consecutively. All tasks in this example are atomic since they do not have WAR dependency on the persistent variables they are accessing, e.g. \emph{x} is only read and \emph{y} is only written within Task 1. Now, suppose three tasks have been virtualized into a single one at run-time, namely Task 4: since \emph{x} is now first \emph{read} and then \emph{written}, a WAR dependency on \emph{x} is introduced dynamically at run-time---unfortunately Task 4 is \emph{no more atomic} since its re-execution will not always produce the same results.

\textbf{Guaranteeing Atomicity of Virtual Tasks:} Although WAR dependencies introduced by the static task division can be eliminated at compile time, e.g.~\cite{alpaca}, those introduced dynamically at run-time, due to task virtualization, cannot. Therefore, we require a \emph{new execution model} that keeps each virtual task atomic by committing the modified persistent variables at the boundary defined dynamically at run-time---keeping the non-volatile memory unmodified upon a power-interrupt and preserving its consistency.

%Existing programming and execution models may require more accesses to non-volatile memory (for multi-versioning)~\cite{dino,chain} or may preclude the use of volatile memory~\cite{ratchet}. Effective use of both volatile and non-volatile memory is important for \emph{efficiency}, because non-volatile memory has higher access energy and latency than SRAM~\cite{nvp}, and \emph{generality}, because devices often have much more FRAM than SRAM (e.g., 64 times more in~\cite{wolverine}). How to connect this trade-off with task virtualization remains an open question.

\textbf{Research Question and Contributions:} Specifically, we ask (i) how to use software support to \emph{dynamically adapt} the effective size of a task, while respecting programmer-specified task atomicity, and (ii) how to minimize run time and energy consumption while \emph{automatically maintaining memory consistency} during execution?

To address this question, we develop {\bf \sys}: a new programming and execution model efficiently executing tasks which avoids non-termination across a range of energy buffer sizes, without burdening the programmer\footnote{Code will be available via \href{http://anonymized.link}{http://anonymized.link}.}. To accomplish this, \sys introduces two new capabilities: 

\begin{itemize}
	\item {\bf Dynamic task coalescing:} \sys's task coalescing mechanism dynamically executes multiple tasks as a single task. A programmer or compiler can specify small tasks that will execute on a device with a small energy buffer. Coalescing tasks allows running the same tasks on a device with a larger energy buffer, avoiding run time overheads associated with ending one task and beginning another.
	%
	\item {\bf Volatile memory virtualization with transiently-powered device:} Colascing through task virtualization requires memory virtualization. \sys's uses SRAM as working memory, which \sys dynamically populates with pages of data from FRAM on demand, during a task's execution. When a task ends, \sys commits dirty pages to FRAM using DMA block copies, ensuring task atomicity and memory consistency. To the best of our knowledge this is the first time when memory virtualization has been demonstrated on a transient power budget.
\end{itemize}

We evaluated \sys end-to-end on a collection of existing benchmarks ~\cite{chain} and new programs, and compare directly to a state-of-the-art task-based runtime~\cite{chain}, showing that \sys has high performance and can flexibly target a variety of platforms without rewriting code. \todo{Provide concrete numeric results supporting this claim}{Amjad/Przemek}

In this work we also address the problem of task construction burden imposed on the programmer. That is, we introduce \sys's compiler that transforms arbitrary C code into a graph of tasks that reflects the original program's control- and data-flow constraints. The compiler identifies data shared by multiple of the newly created tasks and instruments reads and writes of those data so that \sys's memory virtualization mechanism keeps those data consistent. While \sys's complier is still \emph{limited in scope}, e.g. is unable to handle instruction jumps, normalized loops and recursions, it already eliminates the labour-intensive process of writing tasks~\cite{chain,alpaca}, i.e., hours of programmer time.

%\sys's core contribution is the introduction of dynamic task coalescing mechanism. It allows the programmer or a compiler to intuitively decompose the program into small tasks that amortize fixed per-task overheads, yet present no risk of exceeding device energy capacity. As such a decomposition executes, \sys~{\em coalesces dynamically} consecutive tasks. Coalescing elides the commits of coalesced tasks by buffering multiple tasks' updates in an FRAM commit buffer. Periodically, as the span of the coalesced task grows, \sys ends coalescing and commits the state of the coalesced task. If a power failure interrupts a sequence of coalesced tasks, \sys adaptively reduces the number of tasks in that sequence that it will coalesce, committing sooner in future executions. Consequently, \sys's coalescing mechanism allows a program to execute efficiently across a range of energy buffer sizes, avoiding transition overheads in larger buffers, and ensuring progress in smaller buffers.

%We propose and explore two data swapping strategies for intermittently-powered systems based on FRAM/SRAM architecture. The first strategy is {\em demand paging}, which swaps the SRAM page with a new page from FRAM, buffering the swapped-out page until commit. The second strategy is {\em buffered direct access}, which directly reads and writes FRAM relying on dynamic double-buffering to ensure memory is consistent at commit.