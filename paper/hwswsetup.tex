Our hardware and software suite used to evaluate \sys novel intermittent execution model represents the  real implementation of low-power intermittent system. As our implementation makes no system assumptions (we execute \sys model on a \emph{real} hardware powered by \emph{real} intermittent source), therefore promises to generate the ultimate set of \sys performance metrics. 

\subsection{Intermittent Platform and Hardware Toolchain}
\label{sec:results_hardware_software}

We evaluated \sys in two power supply environments: (i)  \emph{intermittent} and (ii) \emph{fixed} (to benchmark the intermittent power supply results). In the experiments with intermittent power supply, we evaluated \sys using a WISP\,5.1~\cite{wisp5,wisp}---energy-harvesting platform build around a MSP430FR5969~\cite{wolverine} MCU with 64\,kB of FRAM. We powered the WISP using RF signal generator~\cite{} emitting 20\,dBm sinusoidal wave at 915\,MHz. \TODO{check if true} Signal generator was connected to the Liard RFMAX S9028PCRJ 8\,dBic antenna~\cite{atlas2015}. For distance-controlled experiments, we affixed WISP at two paper protrusions around 10\,cm from the surface of the table (parallel to, and at the edge of, table surface). Antenna of the signal generator was facing WISP directly and located at three distances, $d=\{15, 30, 50\}$\,cm, from WISP. For task coalescing experiments, an additional distance of $d=60$\,cm was considered. Both antennas were parallel to each other and no physical objects were located between the WISP and signal generator antenna. For continuous power experiments we used an alternative setup with MSP-TS430RGZ48C launchpad~\cite{}, which had the same MCU as WISP. \TODO{Is this the right launchpad model?} In all the experiments the timing measurements were performed using the Saleae~\cite{saleae} logic analyzer. \TODO{How about paging measurements?}

\subsection{Software Benchmarks}
\label{sec:software_benchmarks}

We evaluated \sys using benchmarks that originated from the MiBench testing suite~\cite{mibench,hicks_mibench2_2016}. Specifically, we have implemented three benchmarks used by Chain~\cite{chain} and Alpaca~\cite{alpaca} (source code is accessible~\cite{coala_website}), namely: \textbf{ar}: nearest neighbor classification of randomly generated data modeling a three-axis accelerometer; \textbf{bc}: counts bits in byte pseudo-random string using several algorithms, and compares results for correctness; \textbf{cuckoo}: Bloom-filter-like Cuckoo filter that first hashes a sequence of pseudo-random numbers, then queries the filter to recover the sequence. Additionally, we have ported three new benchmarks from~\cite{mibench}, namely: \textbf{fft}: continuously repeating Fast Fourier Transform on three pre-generated 128 sample vector; and \textbf{sort}: selection sort algorithm on a fixed sequence of numbers. \todo{justify the selection of these benchmarks} All applications are summarized in Table~\ref{table:compiler_result} in terms of memory footprint, source lines of code and number of tasks. \TODO{update table with new data}

All applications were compiled using GCC version 3.8.0. \TODO{Check if this still holds} We compare \sys against state-of-the-art task-based runtime Alpaca~\cite{alpaca}. We do not compare \sys against Chain~\cite{chain}, another task-based system, which is inferior to Alpaca in its performance. Also, we do not compare \sys to checkpointing-based systems, such as DINO~\cite{dino} or Mementos~\cite{mementos} (cf. Section~\ref{sec:background_consistency}), as both of them are build on top LLVM compiler, which performance is inferior to GCC compiler used in the evaluation of \sys \todo{Please check if the LLVM compiler argument makes sense}.