Our hardware and software suite used to evaluate \sys novel intermittent execution model represents the  real implementation of low-power intermittent system. As our implementation makes no system assumptions (we execute \sys model on a \emph{real} hardware powered by \emph{real} intermittent source), therefore promises to generate the ultimate set of \sys performance metrics. 

\subsection{Intermittent Platform and Hardware Toolchain}
\label{sec:results_hardware_software}

We evaluated \sys in two power supply environments: (i)  \emph{intermittent} and (ii) \emph{fixed} (to benchmark the intermittent power supply results). In the experiments with intermittent power supply, we evaluated \sys using a WISP\,5.1~\cite{wisp5,wisp}---energy-harvesting platform build around a MSP430FR5969~\cite{wolverine} MCU with 64\,kB of FRAM. We powered the WISP using RF signal generator~\cite{} \todo{Add reference to signal generator}{Przemek} emitting 20\,dBm sinusoidal wave at 915\,MHz.  Signal generator was connected to the Liard RFMAX S9028PCRJ 8\,dBic antenna~\cite{atlas2015}. For distance-controlled experiments, we affixed WISP at two paper protrusions around 10\,cm from the surface of the table (parallel to, and at the edge of, table surface). Antenna of the signal generator was facing WISP directly and located at three distances, $d=\{15, 30, 50\}$\,cm, from WISP. For task coalescing experiments, an additional distance of $d=60$\,cm was considered. Both antennas were parallel to each other and no physical objects were located between the WISP and signal generator antenna. For continuous power experiments we used an alternative setup with MSP-EXP430FR5969 launchpad~\cite{MSP-EXP430FR5969_launchpad}, which had the same MCU as WISP. In all the experiments the timing measurements were performed using either the Saleae~\cite{saleae} logic analyzer (for intermittent power experiments) or TI Code Composer Studio (for fixed power experiments). Both type of experiments are explained in the following section.

\subsection{Software Benchmarks}
\label{sec:software_benchmarks}

\begin{table}
	\centering
	\footnotesize
	\begin{tabular}{| r | p{0.85\columnwidth} |}
		\hline
		Application & Description \\
		\hline\hline
		\textbf{ar} & nearest neighbor classification of randomly generated data modeling a three-axis accelerometer\\
		\hline
		\textbf{bc} & counts bits in byte pseudo-random string using several algorithms, and compares results for correctness\\
		\hline
		\textbf{cuckoo} & Bloom-filter-like Cuckoo filter: first hashes a sequence of pseudo-random numbers, then queries the filter to recover the sequence\\
		\hline
		\textbf{dijkstra} &  Dijkstra algorithm executed continously on a predefined adjecency matrix\\
		\hline
		\textbf{fft} & continuously repeating Fast Fourier Transform on three pre-generated 128 sample vector\\
		\hline
		\textbf{sort} & selection sort algorithm on a fixed sequence of numbers\\
		\hline
	\end{tabular}
\caption{Suite of benchmark applications used in evaluating \sys performance. All applications have been chosen from MiBench testing suite~\cite{mibench,hicks_mibench2_2016}.\todo{Consider adding more information here}{Brandon}}
\label{table:benchmark_table}
\end{table}

We evaluated \sys using benchmarks that originate from the MiBench testing suite~\cite{mibench,hicks_mibench2_2016}. Set of implemented benchmarks is presented in Table~\ref{table:benchmark_table} and their implementation is available via~\cite{coala_website}. Three benchmarks (\textbf{ar}, \textbf{bc}, \textbf{cuckoo}) were implemented by Chain~\cite{chain} and Alpaca~\cite{alpaca}. Three remaining ones (\textbf{dijkstra}, \textbf{fft}, \textbf{sort}) are added as new. \todo{justify the selection of these benchmarks}{Przemek, Carlo, Amjad, Brandon}

%Specifically, we have implemented three benchmarks used by Chain~\cite{chain} and Alpaca~\cite{alpaca} (source code is accessible~\cite{coala_website}), namely: \textbf{ar}: nearest neighbor classification of randomly generated data modeling a three-axis accelerometer; \textbf{bc}: counts bits in byte pseudo-random string using several algorithms, and compares results for correctness; \textbf{cuckoo}: Bloom-filter-like Cuckoo filter that first hashes a sequence of pseudo-random numbers, then queries the filter to recover the sequence. Additionally, we have ported three new benchmarks from~\cite{mibench}, namely: \textbf{dijkstra}: Dijkstra algorithm executed continously on a predefined adjecency matrix; \textbf{fft}: continuously repeating Fast Fourier Transform on three pre-generated 128 sample vector; and \textbf{sort}: selection sort algorithm on a fixed sequence of numbers. \todo{justify the selection of these benchmarks}

All applications were compiled using GCC version 3.8.0. \todo{Check if this still holds}{Przemek} We compare \sys against state-of-the-art task-based runtime Alpaca~\cite{alpaca}. We do not compare \sys against Chain~\cite{chain}, another task-based system, which is inferior to Alpaca in its performance. Also, we do not compare \sys to checkpointing-based systems, such as DINO~\cite{dino} or Mementos~\cite{mementos} (cf. Section~\ref{sec:background_consistency}), as both of them are build on top LLVM compiler, which performance is inferior to GCC compiler used in the evaluation of \sys \todo{Please check if the LLVM compiler argument makes sense}{Brandon}.