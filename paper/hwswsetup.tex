We prototype \sys and use its API to implement a set of benchmark
applications representative of the embedded domain. We build the applications
and deploy the binaries onto a real energy-harvesting device. In this section
we describe the setup for the experiments presented in
Section~\ref{sec:evaluation} and characterize the benchmark applications.

\subsection{Intermittently-powered Platform and Experimental Setup}
\label{sec:results_hardware_software}

We used two platforms to evaluate \sys: (i) an \emph{intermittently-powered}
energy-harvesting device, and (i) a \emph{continuously-powered} evaluation
board.
%
Every benchmark listed in the next section was run repeatedly on
each platform for a more or less long time window, depending on the benchmark,
to capture a diverse power trace and produce a high number of complete runs of all applications.
The results were averaged across the duration of the experiment.

Experiments on the intermittently-powered platform used the
WISP\,5.1~\cite{wisp5,wisp} RF-powered energy-harvesting device.
%
The WISP contains the MSP430FR5969~\cite{wolverine} MCU with 64\,kB of
non-volatile (FRAM) memory and 2\,kB of volatile (SRAM) memory and was
configured to 1\,MHz clock speed.
%
We powered the WISP using an RF signal generator emitting  a 20-\,dBm sinusoidal wave at 915\,MHz.
The signal generator was connected to the Liard RFMAX S9028PCRJ 8\,dBic
antenna~\cite{atlas2015}.
%
The antenna was oriented towards and in parallel with the WISP antenna, and
no objects obstructed the path.
%
We affixed the WISP with a paper harness at the edge of a table, so that the
WISP was parallel to the table surface at a height of 10\,cm.
%
For distance-controlled experiments we positioned the signal generator
antenna at a fixed distance from the WISP. The distances were $d=\{15, 30,
50\}$\,cm for the main experiments and 60\,cm for the task downscaling
experiments.
%
We chose relatively shorter distances than in a deployment scenario to reduce
charge time and run experiments faster, after having confirmed ourselves a
result also reported in prior work~\cite{alpaca} that the amount of energy
available in one charge cycle --- the sum of stored energy and incoming energy
during execution -- is the same at distances beyond 50~cm where incoming energy
becomes negligible.
%
To obtain execution time, the software toggled GPIO pins at sections of the code
under profile, and the Saleae~\cite{saleae} logic analyzer measured
the interval between edges in the signal.

The experiments which are not sensitive to the behavior of an
energy-harvesting power supply were conducted on an MSP-EXP430FR5969
launchpad~\cite{MSP-EXP430FR5969_launchpad} board powered continuously with a
power supply.
%
This platform has the same MCU as the WISP.
%
Execution time was measured using the clock features in TI Code
Composer Studio IDE version 7.1.
%

\subsection{Software Benchmarks}
\label{sec:software_benchmarks}

\begin{table}
	\centering
	\footnotesize
	\begin{tabular}{| r|r|r | p{0.65\columnwidth} |}
		\hline
		Application & No. tasks & SLOC & Description \\
		\hline\hline
        \textbf{ar} &10 &428 & Activity recognition using a nearest neighbor
classifier with a deterministic input from a randomly-generated sequence of
accelerometer data\\
		\hline
        \textbf{bc} &10 &371 & Counts set bits in a pseudo-random sequence of bytes
using several algorithms and compares results for correctness\\
		\hline
        \textbf{cuckoo} &14 &426 & Inserts and queries a sequence of pseudo-random
values in a Cuckoo filter, a set membership data structure with deletion\\
		\hline
        \textbf{dijkstra} &5 &198 & Dijkstra shortest path algorithm executed on a
predefined adjacency matrix\\
		\hline
        \textbf{fft} &8 &449 & Fast Fourier Transform on three pre-generated 128
sample vectors\\
		\hline
		\textbf{sort} &4 &167 & Selection sort of a fixed sequence of numbers\\
		\hline
	\end{tabular}
\caption{Description and characteristics of benchmark applications used to
evaluate \sys.}
\label{table:benchmark_table}
\end{table}

We evaluated \sys using six benchmarks summarized in
Table~\ref{table:benchmark_table}. %
Three benchmarks (\textbf{ar}, \textbf{bc}, \textbf{cuckoo}) are from prior
work~\cite{chain,alpaca} and three benchmarks (\textbf{dijkstra},
\textbf{fft}, \textbf{sort}) are new implementations of applications
from the MiBench suite~\cite{mibench,hicks_mibench2_2016}. The
\textbf{fft} implementation is based on the Accelerated FFT library from
TI~\cite{ti_dsp}. All applications were compiled using MSP430 GCC~\cite{ti-gcc} version 6.4.0
from toolchain version 5.01.01.00 with \texttt{-O1} as optimization flag.
%
The source code for all benchmarks will be released at \cite{coala_website}.


%Specifically, we have implemented three benchmarks used by Chain~\cite{chain} and Alpaca~\cite{alpaca} (source code is accessible~\cite{coala_website}), namely: \textbf{ar}: nearest neighbor classification of randomly generated data modeling a three-axis accelerometer; \textbf{bc}: counts bits in byte pseudo-random string using several algorithms, and compares results for correctness; \textbf{cuckoo}: Bloom-filter-like Cuckoo filter that first hashes a sequence of pseudo-random numbers, then queries the filter to recover the sequence. Additionally, we have ported three new benchmarks from~\cite{mibench}, namely: \textbf{dijkstra}: Dijkstra algorithm executed continously on a predefined adjecency matrix; \textbf{fft}: continuously repeating Fast Fourier Transform on three pre-generated 128 sample vector; and \textbf{sort}: selection sort algorithm on a fixed sequence of numbers. \todo{justify the selection of these benchmarks}

\textbf{Comparison with Alpaca using the GCC compiler.} We compare
\sys against Alpaca~\cite{alpaca}, a state-of-the-art task-based system for
intermittent computing. For a fair comparison the task decomposition is the
same in \sys and Alpaca implementations of the benchmarks. We compare
both systems using GCC.

Since Alpaca's compiler pass is implemented only for LLVM, we could not use
that implementation to instrument the benchmarks and compile them with GCC.
Instead, we \emph{manually} performed the instrumentation done by the compiler
pass.  We performed the instrumentation at the source level, adding C
statements to the program, instead of at the level of binary
intermediate-representation (LLVM IR) as the compiler pass would. The result is
equivalent from the perspective of overheads. The instrumentation consists of
identifying write-after-read dependencies and adding code and memory
allocations to make a private copy of the affected variables. For array
variables, instrumentation also includes adding code for 
dynamically tracking the array access.

Manual instrumentation of Alpaca code is not a general approach, but only
served as a practical alternative to porting the compiler pass from LLVM to
GCC.
%
Our experiments show that manually-instrumented Alpaca code compiled with GCC
is about 30\%-40\% faster than Alpaca code instrumented automatically by the
Alpaca LLVM compiler pass and compiled with LLVM. This performance difference
is due to the early experimental state of the MSP430 backend for
LLVM~\cite{baghsorkhi_cgo_2018}.
%
By compiling both systems with the same compiler (GCC), we ensure that our
comparison in Section~\ref{sec:evaluation} is fair.

% Unnecessary
%\textbf{Comparison of \sys against other Systems.} We do not compare \sys
%against Chain~\cite{chain}, another task-based system, which is inferior to
%Alpaca in its performance. Also, we do not compare \sys to
%checkpointing-based systems, such as DINO~\cite{dino} or
%Mementos~\cite{mementos} (cf. Section~\ref{sec:background_consistency}).
%\todo{Provide reasons why we do not compare against checkpointing}{Brandon}.
