\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figures/coalEfficiency}
    \caption{Breakdown of overhead for the proposed coalescing strategies
relative to the baseline without coalescing.
NC: no coalescing, EO: energy-oblivious coalescing, HG: history-guided coalescing, WHG:
energy- and task-aware coalescing. All coalescing strategies reduce total
overhead and maximize useful work. HG and WHG perform better than EO.}
	\label{fig:overallOverheadBreakdown}
\end{figure}
% \end{wrapfigure}
%
Building on top of the existing work, we have selected the most efficient run-time library~\cite{alpaca} and compared the performance of \sys against it.
Our evaluation quantitatively demonstrates that \sys (i) \emph{reduces the memory
protection overhead} by coalescing, (ii) \emph{improves execution speed}, in most cases; %
and (iii) by mean of task-downscaling \sys is able to progress where static system stuck in task non-termination loop. 
We have also broken down \sys's overhead to explain the source of \sys's improved performance.
%
\subsection{Characterization of Overhead}
\label{sec:coala_overhead}

\subsubsection{Overhead Reduced by Coalescing}
\label{sec:overhead-coalescing}

For each coalescing strategy from Section~\ref{sec:task_adaptation} (EO, EG,
and WEG) and for a baseline without coalescing (NC), we have measured the time
spent on (i) executing useful task code, including \sys's overhead on access to
protected variables, (ii) re-executing task code after a power failure, and
(iii) committing state to non-volatile memory at the end of each (coalesced)
task.  In this experiment the WISP was 15\,cm away from the signal generator
antenna.
%
The page size was set to the best performing value (32~B for \textbf{bc} and
256~B pages for the rest) identified in detailed evaluation of paging in
Section~\ref{sec:results_memory_management}.

The overhead incurred by each coalescing strategy is broken down in
Fig.~\ref{fig:overallOverheadBreakdown}. Without coalescing enabled (NC), the
re-execution penalty is smallest, because the amount of work that can happen
between commits and may have to be re-executed if interrupted is smallest when
work from multiple static tasks is not combined.
%
However, any gain from a reduced re-execution penalty is canceled out by
the increased commit overhead that is incurred at the end of each static
task.
%
Across all benchmarks, all coalescing strategies reduce more commit overhead
than the re-execution overhead they add.
%
This net overhead reduction is greatest in EG and WEG strategies compared
to the EO strategy. We attribute this discrepancy to EO's slow adjustment
of the target task size without regard to the energy conditions.
%
In the subsequent experiments, we focus on the better-performing EG and WEG
strategies.
%
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figures/overallOverhead}
	\includegraphics[width=\columnwidth]{figures/memAccess}
	\caption{\sys kernel overhead: a breakdown of time spent in
the application and in the runtime (top) and a breakdown of protected memory
accesses into three types with different costs. NC: no coalescing, HG:
history-guided coalescing, WHG: weighted history-guided coalescing.}
	\label{fig:coalEfficiency}
\end{figure}
%
\subsubsection{\sys Kernel Overhead Breakdown}
%
Fig.~\ref{fig:coalEfficiency} (top) breaks down the time spent on executing
useful task code and committing further into time spent by the application and
time spent by \sys's runtime.
%
This dataset is generated in a separate experiment with the same setup as the
previous experiment in Section~\ref{sec:overhead-coalescing}.
%
Consistent with the previous results in
Fig.~\ref{fig:overallOverheadBreakdown}, commit overhead is highest when
coalescing is not used to reduce the number of commits.
%
However, coalescing increases the cost of each access to a protected variable,
since work in the address translation in the paging system increases as the
task size grows.
%
For both coalescing strategies (EG and WEG) accesses to protected variables
constitute about 30\% of the runtime overhead. Dynamic address translation
necessary on each protected access is the most critical bottleneck for \sys.
%
\subsubsection{Protected Memory Accesses Breakdown}
%
Fig.~\ref{fig:coalEfficiency} (bottom) breaks down protected memory accesses
into three categories.
%
Each type of protected access incurs a different amount of overhead: access to
the same page in SRAM as the previous access is cheapest, access to a different
SRAM page has a larger cost, and access to a page that needs to be swapped in
from FRAM into SRAM is most expensive.
%
The results in the figure show that the overwhelming majority of accesses are
of the cheapest kind, which motivated us to optimize this access type in our
implementation (cf. Section~\ref{sec:impl:paging}).
%
Only \textbf{cuckoo}, \textbf{dijkstra}, and \textbf{fft} have non-negligible
number of accesses to a different SRAM page, which is due to the larger working
set and a less regular access pattern in these applications.
%
In general, memory access patterns are shaped by the application, and the more
program state is protected, the higher the rate of page swaps.
%
\subsection{Execution time}
\label{sec:result_coalescing}
%
Having shown in Section~\ref{sec:coala_overhead} that coalescing reduces
overhead, we now investigate the outcome of this reduction on the total
execution time. We first investigate difference variants of \sys and
then compare the best variant to Alpaca~\cite{alpaca}.
%
\subsubsection{\sys Speedup with Coalescing}

Fig.~\ref{fig:coalescing} shows the \sys run time with two coalescing
strategies (EG, WEG) normalized to the run time without coalescing (NC).
%
The experimental setup is the same as that in Section~\ref{sec:coala_overhead}.
%
The results in Fig.~\ref{fig:coalescing} show that all benchmarks
complete faster with coalescing than without coalescing: from 25\% faster
(\textbf{ar}) up to 70\% faster (\textbf{sort}).
%
This speedup is a consequence of the reduced overhead demonstrated in
Section~\ref{sec:coala_overhead}.
%
However, the magnitude of the speedup is (1) highly application-dependent and
(2) largely similar across the two coalescing strategies, with the exception of
\textbf{fft}.
%
In some cases (\textbf{bc}, \textbf{cuckoo}, \textbf{sort}) the Weight Energy-guided
feature in the coalescing algorithm is counter-productive.
%
This occurs in task decompositions with a uniform task energy, where counting
tasks disregarding their energy provides an equal amount of information with a
smaller effort.
%
In \textbf{fft}, tasks are not uniform, and accounting for task energies is
beneficial. In fact, the lack of task energy awareness is detrimental: with EG
\textbf{fft} runs slower than without any coalescing (NC).
%
The speedup is highest for \textbf{bc}, \textbf{cuckoo}, \textbf{dijkstra} and
\textbf{sort}, because their tasks are small and are easily coalesced,
eliminating many unnecessary commits.
%
\begin{figure}
	\includegraphics[width=\columnwidth]{figures/coalStrategies}%
    \caption{\sys's coalescing performance.  Application execution time
with coalescing strategies (EG and WEG) normalized to the execution time
without coalescing.}
	\label{fig:coalescing}
\end{figure}

\begin{figure}
	\includegraphics[width=\columnwidth]{figures/coala_alpaca_gcc}
    \caption{\sys's execution time, normalized to the execution time 
    with Alpaca, as a function of distance to the energy source 
    measured in meter.}
	\label{fig:runtime}
\end{figure}

\subsubsection{Benefits of Adaptive Tasks}
%
We now compare \sys's performance to Alpaca~\cite{alpaca}---a
\emph{non-adaptive} task-based system with tasks fixed at compile time.
Fig.~\ref{fig:runtime} shows the average execution time of each
application for \sys and Alpaca, normalized to the latter. The results show
that \sys provides a performance benefit compared to Alpaca for most
applications. The speedup is greatest for applications with repeated write after
read dependencies throughout their code, particularly involving arrays
(\textit{dijkstra}, \textit{fft} and \textit{sort}). \sys's VMM
successfully amortizes the overhead of protecting memory that is accessed in
such patterns.  In applications without locality among accesses to protected
variables, including array elements, \sys incurs overhead from memory
virtualization that causes its performance to be comparable to (or worse than)
Alpaca (\textit{ar}, \textit{cuckoo}), on average \sys 26\% faster than Alpaca.

The \textit{fft} benchmark did not complete at distances larger than
15\,cm\footnote{At distances greater than 15\,cm, energy incoming during
execution is negligible and stored energy is insufficient to complete some of
the static tasks.} with neither Alpaca, nor \sys \emph{without the downsizing
mechanism}. This is marked with $\infty$ signs in Fig.~\ref{fig:coalescing}
. This non-terminating condition cannot be avoided in Alpaca, due to the
fixed, non-adaptive nature of its tasks.  In contrast, our next experiment
shows how \sys's task downsizing adaptation mechanism eliminates this
non-termination, without changing the static tasks and without recompiling the
application.

In a separate experiment, we have enabled task downsizing in \sys and measured
the execution time for \textit{fft} benchmark at 60\,cm from the signal
generator antenna. \sys executed the
benchmark successfully without (NC) and with coalescing (WEG). The application
took longer to complete than at a shorter distance of 15\,cm in the previous
experiment, because of smaller (negligible) incoming energy at the larger
distance. The variant without coalescing (NC) slowed down by 2.16$\times$ and the
variant with WEG coalescing slowed down by 2.31$\times$ slower, but at both
distances, WEG completed faster than NC.
%
For comparison, the run time with Alpaca is effectively infinite at this
distance, which is a consequence of its non-adaptive static tasks.

\subsection{Virtual Memory Performance}
\label{sec:results_memory_management}

We characterize the performance of \sys's virtual memory sub-system in an
experiment on a continuously-powered evaluation board described in
Section~\ref{sec:methodology}.  Fig.~\ref{fig:page_size} quantifies the
effects of page size.
%
\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figures/page_exec-time}
	\includegraphics[width=\columnwidth]{figures/pagePulls}
    \caption{Effect of page size on execution time (top) and on the number of
page faults (bottom). The execution time is normalized to the lowest in each set
of page sizes.}
	\label{fig:page_size}
\end{figure}
%
\subsubsection{Effect of Page Size on Runtime}
%
Fig.~\ref{fig:page_size} (top) shows the execution time as a function
of page size, normalized to the lowest per-application performance among the
set of page sizes.
The data suggest that there is a page size, distinct from the largest or
smallest, that minimizes execution time. The best page size is not the same for
each application. Nevertheless, if a choice must be made for all applications,
128\,B pages perform well for most of them.
%
\subsubsection{Effect of Page Size on Page Faults}
%
Fig.~\ref{fig:page_size} (bottom) shows the number of page faults as a
function of page size.
%
The smaller the page the more likely that a memory access will land
outside that page and that a new page will need to be swapped in.
%
This trend is visible for all applications, exception \textbf{bc}. The total
amount of data accessed by \textbf{bc}, as well as its working set, is small.
Even with the smallest page, all accesses are contained within that page, and
no page faults occur. Without any page faults to begin with, increasing the
page size result only in overhead.