Advances in processor efficiency along with the development of energy-harvesting systems have created a new category of devices that require neither a battery nor a tethered power supply~\cite{prasad_comst_2014,lucia_snapl_2017,soyata_csm_2016}. These devices operate using ambient energy, such as radio frequency transmissions~\cite{rf_powered_computing_gollakota_2014},
light~\cite{margolies_infocom_2016,margolies_tosn_2016}, and vibration~\cite{gorlatova_sigmetrics_2014}. Incorporating compute, storage, sensing, and communication hardware~\cite{wisp5,moo,capybara}, such devices are a promising technology for use in the Internet of Things~\cite{ku_cst_2016}, in-body~\cite{nadeau_naturebio_2017} and on-body~\cite{bandodkar_electroanalysis_2015} medical systems, and energy-harvesting nano-satellites~\cite{kicksat,capybara}. Energy-harvesting devices create unique challenges because they operate {\em intermittently} when energy is available~\cite{hicks_isca_2017,lucia_snapl_2017}. An energy-harvesting device buffers energy in a small storage capacitor~\cite{gorlatova_tmc_2013,gunduz_commag_2014} and operates when a threshold amount of energy has accumulated. Harvestable energy sources are low-power (e.g., \si{\nano\watt}\ to \si{\micro\watt}) compared to a platform's operating power level (hundreds of \si{\micro\watt}\ to \si{\milli\watt}). A device operates briefly until it depletes its buffered energy, after which, it shuts down and recharges to operate again later---corresponding to the {\em intermittent execution model}~\cite{dino,lucia_snapl_2017} composed of operation---power failure---restart cycles. The recharge and discharge intervals---which correspond to the device's inactive and active time---vary depending on the underlying hardware, such as the size of the energy buffering~\cite{capybara}, and energy conditions. For example, some devices discharge and restart $\approx$10 to $\approx$100 times per second~\cite{tan_infocom_2016,mementos,nvp}.

Upon power failures, a device loses the volatile state in its registers, stack, SRAM, and retains the state of any non-volatile memory, such as FRAM. While capturing periodic checkpoints~\cite{mementos,quickrecall} and sleep scheduling~\cite{dewdrop,hibernus,hibernusplusplus} help preserve execution progress, failures can leave non-volatile state incorrect, partially updated. These inconsistencies cause intermittent execution to deviate from continuously-powered behavior, often leading to an unrecoverable application failure~\cite{dino,edb}. Prior work developed two main approaches to deal with data inconsistency for intermittently-powered devices: (i) \emph{software-based programming and execution models}~\cite{dino,ratchet,chain,alpaca} and (ii) \emph{hardware-based computer architecture support}~\cite{hicks_isca_2017,idetic,nvp}. Complex hardware architectural changes are expensive to design, verify, and manufacture. New hardware architectures are also inapplicable to existing systems~\cite{hicks_isca_2017,nvp}. Software approaches are simpler and applicable to existing devices today. Therefore, this work focuses on software approaches. In particular we address the key limitation of task-based programming and execution model, that is the {\em inflexibility} and \emph{energy-unawareness} of statically decomposing a program into tasks.
%
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/intro-figure.pdf}
    \caption{Task coalescing at runtime reduces time and energy overhead in task-based intermittent programming models by performing fewer commits (denoted as \texttt{C}) of individual tasks (denoted as \texttt{Tx}). On the other hand, task splitting reduces wasted computation and enables termination for bigger tasks.}
    \label{fig:coalesce}
\end{figure}

%\textbf{Task Decomposition of Intermittent Programs.} 

\textbf{Crucial Drawback of Static Task Decomposition.} Task-based programming and execution models require a programmer~\cite{alpaca,chain} or a compiler~\cite{baghsorkhi_cgo_2018} to statically decompose a program into a collection of tasks. A \emph{task}, a top level function, can include arbitrary computation that should be executed despite arbitrarily-timed power failures. The programmer (or a compiler) explicitly expresses task-to-task control flow. Fig.~\ref{fig:coalesce} (left) illustrates how a program's tasks execute and shows how tasks transitions can impose a runtime overhead. At each transition, the system incurs an overhead to track and atomically commit modifications to the non-volatile memory, to maintain consistency of program state~\cite{chain,alpaca}. The more task transitions a program requires, the more commit overhead is incurred by the system at runtime. A programmer may thus create very large tasks in an attempt to reduce task transitions overhead. However, a large task may require more energy to complete than a device's fixed hardware energy buffer can hold, which may lead to a task \emph{non-termination} problem (see Fig.~\ref{fig:coalesce}(right)). To eliminate this risk, existing systems require the programmer to decompose a program into small tasks to preserve execution progress. These constraints on task sizing lead to the following \emph{dilemma}: should large tasks be used, that are efficient but risk non-termination, or small tasks that are guaranteed to complete but incur a high task transition and commit overhead?

\textbf{Challenges and Contributions.} We introduce \sys: a new task-based system that employs \emph{adaptive task size execution by task coalescing and splitting}. By means of this novel technique, small tasks can be executed efficiently by trimming unnecessary overheads \emph{dynamically}, meanwhile avoiding the risk of non-termination. \sys accepts any static decomposition and it coalesces (groups) tasks or splits them (see again Fig.~\ref{fig:coalesce}) based on the estimated energy availability without demanding any hardware support. To the best of our knowledge, \sys is the only system that guarantees forward progress for any statically-defined tasks or energy storage sizes. 

The unique contributions of \sys in relation to challenges of task-based systems revealed by this work are as follows:

\begin{itemize}

\item[C1] \emph{Overcoming Task Transition Overhead:} given unpredictable incoming energy, how to save computation state at task transitions as rare as possible? \sys tries to minimize task transition overhead by estimating energy conditions at run-time using \emph{recent execution history} as a metric.

\item[C2] \emph{Dynamic Memory Consistency Handling:} merging static tasks on the fly rises  the need for dynamic memory consistency handling. This leads to the second challenge: how to dynamically detect inter-coalesced-task data dependencies and ensure efficient protection against power interrupts? \sys addresses this challenge by using a novel approach called \emph{dynamic group privatization}---performing real-time dependency tracking to enable protection on a task transition. Individual variables tracking, however, slows down a system dramatically. Therefore, \sys keeps memory consistent through \emph{memory virtualization} optimized for bulk accesses to task-shared data with high locality.

%Therefore, \sys protects global variables in a batch. Moreover, it optimizes data transfers by using Direct Memory Access (DMA).

\item[C3] \emph{Ensuring Task Termination:} a static task decomposition model assumes that each task can execute to completion. If the hardware energy buffer provides inadequate energy to execute each task to completion, a program will not terminate~\cite{cleancut_2018}. This leads to a third challenge: how to enable the dynamic execution model to progress on a sub-task level? To avoid non-termination under adverse energy conditions, \sys uses a novel timer-based {\em partial task commit} mechanism, inspired by~\cite{ratchet}. Partial execution avoids non-termination by committing the intermediate state of a long-running task that has repeatedly failed and restarted.

\end{itemize}

To asses the benefits of \sys over existing task-based systems, we implemented and tested six benchmarks on a real energy-harvesting platform. Our evaluation not only shows that \sys reduces run time by up to 54\%, and by 26\% on average, but also is able to continue the execution where static task-based systems fail.
 
The rest of this paper is organized as follows. Section~\ref{sec:background} provides background on intermittent computing.
Section~\ref{sec:systemdescription} provides an overview of \sys, while Section~\ref{sec:task_adaptation} describes \sys's task adaptation mechanism. The dynamic group privatization is explained in Section~\ref{sec:memory_virtulaization}. Implementation details of \sys are given in Section~\ref{sec:implementation}. Sections~\ref{sec:methodology} and~\ref{sec:evaluation} describe \sys's evaluation methodology and results. Section~\ref{sec:related_work} positions \sys in the context of related work and Section~\ref{sec:conclusions} concludes and discusses future work.